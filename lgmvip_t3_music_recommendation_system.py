# -*- coding: utf-8 -*-
"""LGMVIP_T3_Music_Recommendation_System.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oXOCMpp6MExjJuAuW2cYqSHvh2M70-ze

<center><h1><b><font color="blue"> Let's Grow More </b></h1></font>
    <h2> Task 3: Music Recommendation </h2>
    <h3> by -Jalaj Singh </h3></center>
"""

#The files provided are zipped hence need to unzip and extract our datasets

import py7zr

df1 = py7zr.SevenZipFile("train.csv.7z", mode='r')
df1.extractall(path='Dataset')
df1.close()

"""- If I would want to extract in same directory instead of dataset folder then
-  Unzipping the "members.csv.7z" file in the same folder
    - df2 = py7zr.SevenZipFile("members.csv.7z", mode='r')
    - df2.extractall()
    - df2.close()
"""

df2 = py7zr.SevenZipFile("members.csv.7z",mode='r')
df2.extractall(path='Dataset')
df2.close()

df3 = py7zr.SevenZipFile("song_extra_info.csv.7z",mode='r')
df3.extractall(path='Dataset')
df3.close()

df4 = py7zr.SevenZipFile("songs.csv.7z",mode='r')
df4.extractall(path='Dataset')
df4.close()

df5 = py7zr.SevenZipFile("test.csv.7z",mode='r')
df5.extractall(path='Dataset')
df5.close()

df6 = py7zr.SevenZipFile("sample_submission.csv.7z",mode='r')
df6.extractall(path='Dataset')
df6.close()

"""- 'py7zr' library is used to unzip a 7z compressed file and extract its contents to a specified directory

- the above code unzips the files like "train.csv.7z" file and extracts its contents to the "Dataset" directory. After running this code, we can find the "train.csv" file (or whatever file(s) were inside the 7z archive) in the "Dataset" directory.

#### Importing other necessary libraries
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

"""#### Loading dataset"""

train = pd.read_csv("Dataset/train.csv")
songs = pd.read_csv("Dataset/songs.csv")
members = pd.read_csv("Dataset/members.csv")
song_info = pd.read_csv("Dataset/song_extra_info.csv")
test = pd.read_csv("Dataset/test.csv")
sample_submission = pd.read_csv("Dataset/sample_submission.csv")

train.shape

songs.shape

members.shape

song_info.shape

test.shape

sample_submission.shape

"""- Considering only 20k tuples from required dataset"""

train_df = pd.read_csv("Dataset/train.csv",nrows=20000)
songs_df = pd.read_csv("Dataset/songs.csv",nrows=20000)
members_df = pd.read_csv("Dataset/members.csv")
song_extra_info_df = pd.read_csv("Dataset/song_extra_info.csv")
test_df = pd.read_csv("Dataset/test.csv")
sample_submission_df = pd.read_csv("Dataset/sample_submission.csv",nrows=20000)

train_df.head()

songs_df.head()

test_df.head()

members_df.head()

song_extra_info_df.head()

sample_submission_df.head()

"""#### Data Preprocessing

- Checking for null values
"""

train_df.isna().sum()

"""- We'll use only msno and song_id attribute, so no missing values
- Removing all attributes except msno and song_id
"""

train_df=train_df.drop(['source_system_tab','source_screen_name','source_type'],axis=1)
train_df.head()

"""- Renaming msno as user_id"""

train_df.rename(columns={'msno':'user_id'},inplace=True)
train_df.head()

train_df.shape

songs_df.isna().sum()

songs_df.info()

"""- Filling the missing values"""

songs_df['genre_ids'].fillna(' ',inplace=True)
songs_df['composer'].fillna(' ',inplace=True)
songs_df['lyricist'].fillna(' ',inplace=True)

"""- We'll use all attributes except song_length and language"""

songs_df.isnull().sum()

song_extra_info_df.isna().sum()

"""- We'll not use isrc attribute in our main dataset so, no need to clean it

"""

train_df.head()

# Explore the target variable distribution
sns.countplot(x='target', data=train_df)
plt.title('Target Variable Distribution')
plt.show()

songs_df.head()

# Explore the distribution of song lengths in songs_df
sns.histplot(songs_df['song_length'], bins=20)
plt.title('Song Length Distribution')
plt.show()

# Explore the distribution of language in songs_df
sns.countplot(x='language', data=songs_df)
plt.title('Language Distribution')
plt.show()

"""#### Preparing Main Dataset
- Creating new dataframe main_df by merging train_df and songs_df
"""

main_df = train_df
main_df.head()

"""- Merging main_df and song_df

"""

main_df = main_df.merge(songs_df, on='song_id')

main_df.head()

"""- Dropping song_length and language columns"""

main_df = main_df.drop(['song_length', 'language'],axis=1)
main_df.head()

song_extra_info_df.head()

"""- Merging song_extra_info_df and main_df

- Droping isrc attribute.
"""

main_df = main_df.merge(song_extra_info_df, on='song_id').drop('isrc', axis=1)
main_df.head()

"""- Renaming name attribute with song_name"""

main_df.rename(columns={'name':'song_name'}, inplace=True)
main_df.head()

"""#### Performing Preprocessing for our constructed main_df dataset"""

main_df['genre_ids'].value_counts()

"""- There are lots of | in genre_ids. Replacing | with ' '
- Replacing '|' with a space ' ', the different genre IDs are combined into a single string, which can be more convenient for further analysis or modeling.
"""

main_df['genre_ids'] = main_df['genre_ids'].str.replace('|', ' ', regex=True)
main_df['genre_ids'].value_counts().tail()

main_df.shape

"""- Here in attributes (artist_name, commposer and lyricist) have '/' and '|' symbols.
- Replacing it with " "
"""

main_df['artist_name'] = main_df['artist_name'].str.replace( '|',' ', regex=True)
main_df['artist_name'] = main_df['artist_name'].str.replace( '/',' ',  regex=True)

main_df['lyricist'] = main_df['lyricist'].str.replace( '|',' ', regex=True)
main_df['lyricist'] = main_df['lyricist'].str.replace( '/',' ', regex=True)

main_df['composer'] = main_df['composer'].str.replace( '|',' ', regex=True)
main_df['composer'] = main_df['composer'].str.replace( '/',' ', regex=True)

main_df.sample(10)

"""- Now, also converting them into Lower Case"""

main_df['artist_name']  = main_df['artist_name'].str.lower()
main_df['lyricist']  = main_df['lyricist'].str.lower()
main_df['composer']  = main_df['composer'].str.lower()

main_df.sample(10)

"""- Creating new column song_details with values concatenating corresoponding values of attributes artist_name, composer, lyricist ."""

main_df['songs_details']=main_df['artist_name']+''+main_df['composer']+main_df['lyricist']
main_df.head()

"""- To find how many timmes each user ID appears in the dataset"""

main_df.user_id.value_counts()

"""#### Computing Similarities between songs using cosine similarities

- Before computing, cosine similaritiy, we need to remove duplicate values in song_details
"""

main_df.duplicated().sum()

"""- "As main_df contains user_id, so there will be rarely duplicated tuples. Here, it's zero."
The DataFrame "main_df" contains the 'user_id' column, which uniquely identifies each row as it represents different users. Since different users can listen to the same song, there might be multiple rows with the same 'song_id' but different 'user_id'. Therefore, there could be duplicate 'song_details' for different users. However, since the 'user_id' column is included, the tuples (rows) are unique due to the distinct 'user_id' values. Thus, the number of duplicated tuples (rows) in "main_df" is zero.

- "As different users can listen to the same song, so we need to remove user_id and see for duplicate tuples in our dataset."
To calculate the similarity between songs based on their attributes such as 'genre_ids', 'artist_name', 'composer', and 'lyricist', it's essential to remove the 'user_id' column, as the user information is not relevant for the similarity calculation. The 'user_id' column uniquely identifies the interaction between a specific user and a specific song, but it is not relevant for comparing the songs themselves.

- Once the 'user_id' column is removed, the DataFrame "temp_df" contains only the song-related attributes, and there might be some duplicate rows (tuples) if different users have listened to the same song. These duplicates need to be removed to ensure that each unique song is represented only once when calculating cosine similarity.

- After removing the 'user_id' column and eliminating any duplicate rows, the remaining rows in "temp_df" represent unique songs with their associated attributes. This clean dataset is then used to compute cosine similarity between songs based on their 'song_details', which is a combination of 'genre_ids', 'artist_name', 'composer', and 'lyricist'.

- By focusing on the songs' characteristics rather than individual user interactions, the music recommendation system can provide more accurate and general song recommendations that are based on the intrinsic properties of the songs themselves.

#### Now, make a copy of main_df to temp_df so that we can drop user_id from it and use it on cosine similarity
"""

temp_df = main_df.copy() # Making duplicate of Original dataset.
temp_df.head()

"""- Now checking duplicated values before computing cosine similarities"""

temp_df.songs_details.duplicated().sum()

temp_df.shape

temp_df.duplicated().sum()

temp_df = temp_df.drop('user_id', axis=1) # Removing user_id from temp_df

temp_df.head()

temp_df.duplicated().sum()

"""- There are 715 duplicated tuples

#### Reason for dropping user_id
- Once the 'user_id' column is removed, the DataFrame "temp_df" contains only the song-related attributes, and there might be some duplicate rows (tuples) if different users have listened to the same song. These duplicates need to be removed to ensure that each unique song is represented only once when calculating cosine similarity.
"""

temp_df = temp_df.drop_duplicates()
temp_df.shape

"""- After dropping duplicate values
- 1509-715 = 794
- Now, there are 794 unique values
"""

temp_df.head()

temp_df.reset_index(inplace=True)

temp_df.head()

# Top 10 most common artists
top_artists = temp_df['artist_name'].value_counts().nlargest(10)
plt.figure(figsize=(10, 6))
sns.barplot(x=top_artists.index, y=top_artists.values)
plt.title('Top 10 Most Common Artists')
plt.xticks(rotation=45, ha='right')
plt.xlabel('Artist Name')
plt.ylabel('Count')
plt.show()

# Top 10 most common composers
top_composers = temp_df['composer'].value_counts().nlargest(10)
plt.figure(figsize=(10, 6))
sns.barplot(x=top_composers.index, y=top_composers.values)
plt.title('Top 10 Most Common Composers')
plt.xticks(rotation=45, ha='right')
plt.xlabel('Composer')
plt.ylabel('Count')
plt.show()

# Top 10 most common lyricists
top_lyricists = temp_df['lyricist'].value_counts().nlargest(10)
plt.figure(figsize=(10, 6))
sns.barplot(x=top_lyricists.index, y=top_lyricists.values)
plt.title('Top 10 Most Common Lyricists')
plt.xticks(rotation=45, ha='right')
plt.xlabel('Lyricist')
plt.ylabel('Count')
plt.show()

# Word cloud for 'song_details'
from wordcloud import WordCloud

songs_details_text = ' '.join(temp_df['songs_details'].dropna())
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(songs_details_text)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud for Song Details')
plt.show()

"""- In the above code, we are creating a word cloud visualization for the 'song_details' text data in the DataFrame "temp_df". A word cloud is a graphical representation of the most frequent words in a given text corpus, where the size of each word is proportional to its frequency. Word clouds are commonly used to visually display the most prominent words in a text dataset.

#### Feature extraction using TF-IDF
"""

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf=TfidfVectorizer(analyzer='word',stop_words='english')
tfidf_matrix=tfidf.fit_transform(temp_df['songs_details'])

"""- At first we import the TfidfVectorizer class from the sklearn.feature_extraction.text module, which is a utility for text feature extraction using the TF-IDF representation.

- After that we create an instance of the TfidfVectorizer class with the specified parameters:
analyzer='word': This specifies that the vectorizer should consider words as the individual features for representing the text data.
stop_words='english': This tells the vectorizer to remove common English stop words (e.g., 'the', 'a', 'an', 'and', etc.) from the text data, as they often do not carry significant meaning for analysis.

- tfidf_matrix = tfidf.fit_transform(temp_df['songs_details']): This line applies the fit_transform() method of the TfidfVectorizer to the 'songs_details' column of the DataFrame "temp_df". The fit_transform() method first learns the vocabulary from the input text (i.e., 'songs_details') and then transforms the text data into a numerical feature matrix using the TF-IDF representation.

- fit_transform(): This method learns the vocabulary of the text data and then converts it into a numerical feature matrix. The result, tfidf_matrix, is a sparse matrix that represents the TF-IDF values of the 'songs_details' text data.

The TF-IDF representation captures the importance of each word in each 'songs_details' text relative to the entire corpus. The more frequently a word appears in a 'songs_details', the higher its TF-IDF score will be. Additionally, words that appear frequently across all 'songs_details' will have lower TF-IDF scores.

This tfidf_matrix can be used as input for various machine learning algorithms to analyze and make recommendations based on the similarity of the songs' characteristics represented by the TF-IDF vectors.

- TF-IDF (Term Frequency-Inverse Document Frequency) vectorization is used before computing cosine similarity in natural language processing and text analysis tasks. Here's why TF-IDF is typically used before cosine similarity:

- Text Representation: In natural language processing, text data needs to be represented as numerical features to be used in machine learning models. TF-IDF is a popular technique to convert raw text data into a numerical vector representation that retains the semantic meaning of the words.

- TF-IDF Weights: TF-IDF calculates the importance of each word in a document relative to the entire corpus. It assigns higher weights to words that appear frequently in a document but are rare across all documents in the corpus. Words that are common across all documents receive lower weights. This way, the TF-IDF vector captures the relative importance of words in each document.

- Dimensionality Reduction: The TF-IDF vector representation reduces the dimensionality of the text data. Each document is represented by a numerical vector, and the vector's dimension is equal to the vocabulary size (the number of unique words) in the corpus.

- Cosine Similarity: Cosine similarity is a metric used to measure the similarity between two vectors. It calculates the cosine of the angle between two vectors and ranges from -1 to 1. A value close to 1 indicates high similarity, while a value close to -1 indicates dissimilarity. A value close to 0 indicates orthogonality (no similarity).

- Cosine Similarity on TF-IDF: After TF-IDF vectorization, each document is represented by a numerical vector in the TF-IDF space. Cosine similarity is then calculated between pairs of vectors representing different documents. The similarity score reflects the semantic similarity of the text content in the documents.

- Text Similarity and Recommendations: In the context of a music recommendation system or text similarity tasks, cosine similarity on TF-IDF vectors helps identify similar songs based on their lyrics, artists, and other text-based attributes. By measuring the cosine similarity between TF-IDF vectors, you can recommend songs that are semantically similar in terms of their textual content.

- Overall, TF-IDF vectorization is an essential step in representing text data for various natural language processing tasks, including music recommendation systems. It allows for meaningful comparisons between documents based on their text content, and cosine similarity on TF-IDF vectors helps identify similar songs for music recommendation purposes.

#### Now implementing Cosine Similarity
"""

from sklearn.metrics.pairwise import cosine_similarity

cosine_similarities=cosine_similarity(tfidf_matrix)

cosine_similarities

cosine_similarities.shape

cosine_similarities[0]

sorted(list(enumerate(cosine_similarities[0])),reverse=True,key=lambda x:x[1])[1:6]

"""- cosine_similarities: This is likely a matrix containing the cosine similarity scores between different songs. For example, if there are N songs, cosine_similarities will be an NxN matrix where each entry represents the cosine similarity score between a pair of songs.

- cosine_similarities[0]: This part extracts the first row of the cosine_similarities matrix. This row represents the cosine similarity scores between the first song and all other songs in the dataset.

- enumerate(cosine_similarities[0]): The enumerate() function is used to iterate over the elements of the first row of cosine_similarities. It returns pairs of (index, value), where index is the index of the element in the row, and value is the cosine similarity score.

- sorted(...): This part sorts the enumerated cosine similarity scores in descending order based on the similarity scores (the second element of each pair). The reverse=True argument indicates that the sorting should be in descending order (highest similarity scores first).

- key=lambda x: x[1]: This is a lambda function used as the key for sorting. It specifies that the sorting should be based on the second element of each pair (i.e., the cosine similarity score).

- [1:6]: Finally, this part slices the sorted list to get the top 5 most similar songs (excluding the first element, which would be the song's similarity to itself since the similarity of a song to itself is always 1).

- To summarize, the code snippet is extracting the cosine similarity scores between the first song and all other songs in the dataset, then sorting these similarity scores in descending order to find the top 5 most similar songs to the first song. The result will be a list of pairs, where each pair contains the index of a song and its corresponding cosine similarity score with the first song.
"""

temp_df.head()

#In which you can recommend only index
def recommend(song):
    song_index=temp_df[temp_df['song_name']==song].index[0]
    distances=cosine_similarities[song_index]
    song_list=sorted(list(enumerate(cosine_similarities[0])),reverse=True,key=lambda x:x[1])[1:6]
    for i in song_list:
        print(i[0])

"""- Here, the recommend(song) function takes a song name as input, finds the index of that song in the DataFrame "temp_df", retrieves its cosine similarity scores with all other songs, and then recommends the top 5 most similar songs based on their indices in "temp_df". The indices are then printed as recommendations. Note that these indices can be used to retrieve the full song details from the original dataset."""

recommend('Panda')

"""#### User-based Recommender
- Recommending song name instead of indexes
"""

def recommend(song):
    song_index=temp_df[temp_df['song_name']==song].index[0]
    distances=cosine_similarities[song_index]
    song_list=sorted(list(enumerate(distances)),reverse=True,key=lambda x:x[1])[1:10]
    for i in song_list:
        print(temp_df.iloc[i[0]].song_name)

recommend('Panda')